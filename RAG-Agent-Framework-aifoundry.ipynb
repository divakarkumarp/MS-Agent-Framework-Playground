{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1663f221",
   "metadata": {},
   "source": [
    "### RAG - Agent Framework - AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cc5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "! pip install chromadb sentence-transformers agent-framework openai pypdf python-docx -q\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb6979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from typing import Annotated, List, Dict\n",
    "from pydantic import Field\n",
    "from agent_framework import ChatAgent\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fd4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key configured!\n"
     ]
    }
   ],
   "source": [
    "# Configure API Key\n",
    "# Replace with your actual OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ API Key configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb875f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DocumentProcessor class defined with file loading support!\n"
     ]
    }
   ],
   "source": [
    "# Define Document Processor with File Loading\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Smart document processing with semantic chunking and file loading\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def load_from_folder(self, folder_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load all documents from a folder.\n",
    "        Supports: .txt, .md, .pdf, .docx files\n",
    "        \"\"\"\n",
    "        folder = Path(folder_path)\n",
    "        \n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "        \n",
    "        documents = []\n",
    "        supported_extensions = ['.txt', '.md', '.pdf', '.docx']\n",
    "        \n",
    "        # Get all files with supported extensions\n",
    "        for file_path in folder.iterdir():\n",
    "            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n",
    "                try:\n",
    "                    content = self._read_file(file_path)\n",
    "                    if content.strip():  # Only add non-empty documents\n",
    "                        documents.append({\n",
    "                            'content': content,\n",
    "                            'source': str(file_path),\n",
    "                            'filename': file_path.name,\n",
    "                            'file_type': file_path.suffix[1:]  # Remove the dot\n",
    "                        })\n",
    "                        print(f\"  ‚úì Loaded: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚úó Error loading {file_path.name}: {e}\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _read_file(self, file_path: Path) -> str:\n",
    "        \"\"\"Read file content based on file type\"\"\"\n",
    "        suffix = file_path.suffix.lower()\n",
    "        \n",
    "        if suffix in ['.txt', '.md']:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        elif suffix == '.pdf':\n",
    "            try:\n",
    "                import pypdf\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    pdf = pypdf.PdfReader(f)\n",
    "                    content = []\n",
    "                    for page in pdf.pages:\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            content.append(text)\n",
    "                    return \"\\n\\n\".join(content)\n",
    "            except ImportError:\n",
    "                print(\"  ‚ö†Ô∏è  pypdf not installed. Install with: pip install pypdf\")\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error reading PDF: {e}\")\n",
    "                return \"\"\n",
    "        \n",
    "        elif suffix == '.docx':\n",
    "            try:\n",
    "                import docx\n",
    "                doc = docx.Document(file_path)\n",
    "                content = []\n",
    "                for para in doc.paragraphs:\n",
    "                    if para.text.strip():\n",
    "                        content.append(para.text)\n",
    "                return \"\\n\\n\".join(content)\n",
    "            except ImportError:\n",
    "                print(\"  ‚ö†Ô∏è  python-docx not installed. Install with: pip install python-docx\")\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error reading DOCX: {e}\")\n",
    "                return \"\"\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def semantic_chunk(self, text: str, metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Semantic chunking that preserves context\"\"\"\n",
    "        chunks = []\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            if len(current_chunk) + len(para) > self.chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        'content': current_chunk.strip(),\n",
    "                        'metadata': {\n",
    "                            **metadata,\n",
    "                            'chunk_id': chunk_id,\n",
    "                            'char_count': len(current_chunk)\n",
    "                        },\n",
    "                        'id': self._generate_id(current_chunk, metadata['source'], chunk_id)\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                    \n",
    "                    overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + para\n",
    "                else:\n",
    "                    current_chunk = para\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + para if current_chunk else para\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                'content': current_chunk.strip(),\n",
    "                'metadata': {\n",
    "                    **metadata,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'char_count': len(current_chunk)\n",
    "                },\n",
    "                'id': self._generate_id(current_chunk, metadata['source'], chunk_id)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _generate_id(self, content: str, source: str, chunk_id: int) -> str:\n",
    "        \"\"\"Generate unique ID for chunk\"\"\"\n",
    "        unique_str = f\"{source}_{chunk_id}_{content[:50]}\"\n",
    "        return hashlib.md5(unique_str.encode()).hexdigest()\n",
    "\n",
    "print(\"‚úÖ DocumentProcessor class defined with file loading support!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0b0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VectorDBManager class defined with open-source embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Define Vector Database Manager with Open-Source Embeddings\n",
    "class VectorDBManager:\n",
    "    \"\"\"Manage ChromaDB vector database with open-source embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 collection_name: str = \"rag_knowledge_base\",\n",
    "                 persist_directory: str = \"./chroma_db\",\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 use_local_embeddings: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize ChromaDB with open-source embedding models\n",
    "        \n",
    "        Embedding Model Options (all open-source):\n",
    "        - 'all-MiniLM-L6-v2': Fast, 384 dim, best for general use (DEFAULT)\n",
    "        - 'all-mpnet-base-v2': Better accuracy, 768 dim, slower\n",
    "        - 'paraphrase-multilingual-MiniLM-L12-v2': Multilingual support, 384 dim\n",
    "        - 'BAAI/bge-small-en-v1.5': High quality, 384 dim\n",
    "        - 'BAAI/bge-base-en-v1.5': Better quality, 768 dim\n",
    "        - 'intfloat/e5-small-v2': Efficient, 384 dim\n",
    "        - 'intfloat/e5-base-v2': Better quality, 768 dim\n",
    "        \"\"\"\n",
    "        # Initialize ChromaDB with persistence\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        self.embedding_model_name = embedding_model\n",
    "        \n",
    "        if use_local_embeddings:\n",
    "            # Use sentence-transformers (fully open-source, runs locally)\n",
    "            print(f\"üì• Loading open-source embedding model: {embedding_model}\")\n",
    "            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "                model_name=embedding_model,\n",
    "                device='cpu'  # Change to 'cuda' if you have GPU\n",
    "            )\n",
    "            print(f\"‚úÖ Model loaded successfully (running locally)\")\n",
    "        else:\n",
    "            # Fallback to default ChromaDB embeddings\n",
    "            self.embedding_function = embedding_functions.DefaultEmbeddingFunction()\n",
    "            print(\"‚úÖ Using default embeddings\")\n",
    "        \n",
    "        # Create or get collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_function,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Vector DB initialized: {collection_name}\")\n",
    "        print(f\"üìä Current documents: {self.collection.count()}\")\n",
    "    \n",
    "    def add_documents(self, chunks: List[Dict]):\n",
    "        \"\"\"Add document chunks to vector database\"\"\"\n",
    "        documents = [chunk['content'] for chunk in chunks]\n",
    "        metadatas = [chunk['metadata'] for chunk in chunks]\n",
    "        ids = [chunk['id'] for chunk in chunks]\n",
    "        \n",
    "        print(f\"üîÑ Generating embeddings for {len(documents)} chunks...\")\n",
    "        \n",
    "        # Batch insert\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_meta = metadatas[i:i + batch_size]\n",
    "            batch_ids = ids[i:i + batch_size]\n",
    "            \n",
    "            self.collection.add(\n",
    "                documents=batch_docs,\n",
    "                metadatas=batch_meta,\n",
    "                ids=batch_ids\n",
    "            )\n",
    "            \n",
    "            print(f\"  ‚úì Processed batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}\")\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(documents)} chunks to vector database\")\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"Semantic search in vector database\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            formatted_results.append({\n",
    "                'content': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'distance': results['distances'][0][i] if 'distances' in results else None\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def clear_collection(self):\n",
    "        \"\"\"Clear all documents from collection\"\"\"\n",
    "        self.client.delete_collection(self.collection.name)\n",
    "        print(f\"üóëÔ∏è  Cleared collection: {self.collection.name}\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get collection statistics\"\"\"\n",
    "        count = self.collection.count()\n",
    "        return {\n",
    "            'total_documents': count,\n",
    "            'collection_name': self.collection.name,\n",
    "            'embedding_model': self.embedding_model_name\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ VectorDBManager class defined with open-source embeddings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9958d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAgent class defined!\n"
     ]
    }
   ],
   "source": [
    "# Define RAG Agent\n",
    "class RAGAgent:\n",
    "    \"\"\"RAG Agent with Microsoft Agent Framework + ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db: VectorDBManager, model: str = \"gpt-4o-mini\"):\n",
    "        self.vector_db = vector_db\n",
    "        self.model = model\n",
    "        self.agent = None\n",
    "        \n",
    "    def create_search_tool(self):\n",
    "        \"\"\"Create vector search tool for the agent\"\"\"\n",
    "        \n",
    "        def search_knowledge_base(\n",
    "            query: Annotated[str, Field(description=\"The user's question or search query\")]\n",
    "        ) -> str:\n",
    "            \"\"\"\n",
    "            Search the technical documentation knowledge base.\n",
    "            Returns relevant information to answer the user's question.\n",
    "            \"\"\"\n",
    "            # Perform vector search\n",
    "            results = self.vector_db.search(query, n_results=3)\n",
    "            \n",
    "            if not results:\n",
    "                return \"No relevant information found in the knowledge base.\"\n",
    "            \n",
    "            # Format results with sources\n",
    "            formatted_response = []\n",
    "            for idx, result in enumerate(results, 1):\n",
    "                metadata = result['metadata']\n",
    "                content = result['content']\n",
    "                \n",
    "                formatted_response.append(\n",
    "                    f\"**Source {idx}: {metadata.get('filename', 'Unknown')}**\\n\"\n",
    "                    f\"{content}\\n\"\n",
    "                    f\"_[Chunk ID: {metadata.get('chunk_id', 'N/A')}, \"\n",
    "                    f\"Relevance Score: {1 - result['distance']:.3f}]_\"\n",
    "                )\n",
    "            \n",
    "            return \"\\n\\n---\\n\\n\".join(formatted_response)\n",
    "        \n",
    "        return search_knowledge_base\n",
    "    \n",
    "    def initialize_agent(self):\n",
    "        \"\"\"Initialize the RAG agent\"\"\"\n",
    "        search_tool = self.create_search_tool()\n",
    "        \n",
    "        self.agent = ChatAgent(\n",
    "            chat_client=OpenAIChatClient(),\n",
    "            instructions=\"\"\"You are an expert technical documentation assistant.\n",
    "            \n",
    "When answering questions:\n",
    "1. Always use the search_knowledge_base tool to find relevant information\n",
    "2. Base your answers primarily on the retrieved context\n",
    "3. Cite the specific sources (by Source number) for your information\n",
    "4. If the context doesn't contain enough information, acknowledge this\n",
    "5. Be concise but thorough\n",
    "6. Use technical terminology appropriately\n",
    "7. Provide code examples when available in the context\n",
    "\n",
    "Format your responses in a clear, structured way with proper markdown.\"\"\",\n",
    "            tools=[search_tool],\n",
    "            model=self.model\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ RAG Agent initialized\")\n",
    "    \n",
    "    async def query(self, question: str) -> str:\n",
    "        \"\"\"Query the RAG agent\"\"\"\n",
    "        if not self.agent:\n",
    "            self.initialize_agent()\n",
    "        \n",
    "        result = await self.agent.run(question)\n",
    "        return result.text\n",
    "\n",
    "print(\"‚úÖ RAGAgent class defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a5305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading documents from: ./docs\n",
      "\n",
      "  ‚úì Loaded: RAG_Q&A.pdf\n",
      "\n",
      "‚úÖ Successfully loaded 1 documents:\n",
      "\n",
      "Document Summary:\n",
      "------------------------------------------------------------\n",
      "  üìÑ RAG_Q&A.pdf\n",
      "     Type: PDF\n",
      "     Size: 57877 chars, ~6894 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load documents from docs folder\n",
    "DOCS_FOLDER = \"./docs\"  # Change this to your folder path\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(DOCS_FOLDER):\n",
    "    print(f\"‚ö†Ô∏è  Folder '{DOCS_FOLDER}' not found. Creating it...\")\n",
    "    os.makedirs(DOCS_FOLDER)\n",
    "    print(f\"‚úÖ Created '{DOCS_FOLDER}' folder\")\n",
    "    print(f\"\\nüìå Please add your documents (.txt, .md, .pdf, .docx) to the '{DOCS_FOLDER}' folder\")\n",
    "    print(\"   Then re-run this cell.\")\n",
    "    documents = []\n",
    "else:\n",
    "    print(f\"üìÇ Loading documents from: {DOCS_FOLDER}\\n\")\n",
    "    \n",
    "    # Initialize document processor\n",
    "    doc_processor_temp = DocumentProcessor()\n",
    "    \n",
    "    # Load all documents\n",
    "    documents = doc_processor_temp.load_from_folder(DOCS_FOLDER)\n",
    "    \n",
    "    if documents:\n",
    "        print(f\"\\n‚úÖ Successfully loaded {len(documents)} documents:\")\n",
    "        print(\"\\nDocument Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        for doc in documents:\n",
    "            word_count = len(doc['content'].split())\n",
    "            print(f\"  üìÑ {doc['filename']}\")\n",
    "            print(f\"     Type: {doc['file_type'].upper()}\")\n",
    "            print(f\"     Size: {len(doc['content'])} chars, ~{word_count} words\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No supported documents found in '{DOCS_FOLDER}'\")\n",
    "        print(\"   Supported formats: .txt, .md, .pdf, .docx\")\n",
    "        print(f\"\\n   Please add documents to '{DOCS_FOLDER}' and re-run this cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f24709fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RAG System with Open-Source Embeddings...\n",
      "\n",
      "üìå Selected embedding model: all-MiniLM-L6-v2\n",
      "\n",
      "üì• Loading open-source embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully (running locally)\n",
      "‚úÖ Vector DB initialized: tech_docs_kb\n",
      "üìä Current documents: 0\n",
      "\n",
      "‚úÖ System components initialized!\n",
      "üîí All processing runs locally - no data sent to external APIs\n"
     ]
    }
   ],
   "source": [
    "# Initialize Document Processor and Vector DB with Open-Source Embeddings\n",
    "if not documents:\n",
    "    print(\"‚ö†Ô∏è  No documents loaded. Please run Cell 7 first.\")\n",
    "else:\n",
    "    print(\"üöÄ Initializing RAG System with Open-Source Embeddings...\\n\")\n",
    "    \n",
    "    # Initialize document processor\n",
    "    doc_processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # ========== CHOOSE YOUR EMBEDDING MODEL ==========\n",
    "    # Options (all 100% open-source and run locally):\n",
    "    \n",
    "    # OPTION 1: Fast and lightweight (RECOMMENDED for most cases)\n",
    "    embedding_model = \"all-MiniLM-L6-v2\"  # 384 dim, 22M params\n",
    "    \n",
    "    # OPTION 2: Better accuracy\n",
    "    # embedding_model = \"all-mpnet-base-v2\"  # 768 dim, 110M params\n",
    "    \n",
    "    # OPTION 3: State-of-the-art (BGE models from BAAI)\n",
    "    # embedding_model = \"BAAI/bge-small-en-v1.5\"  # 384 dim\n",
    "    # embedding_model = \"BAAI/bge-base-en-v1.5\"   # 768 dim\n",
    "    # embedding_model = \"BAAI/bge-large-en-v1.5\"  # 1024 dim (best quality)\n",
    "    \n",
    "    # OPTION 4: E5 models (efficient)\n",
    "    # embedding_model = \"intfloat/e5-small-v2\"  # 384 dim\n",
    "    # embedding_model = \"intfloat/e5-base-v2\"   # 768 dim\n",
    "    \n",
    "    # OPTION 5: Multilingual support\n",
    "    # embedding_model = \"paraphrase-multilingual-MiniLM-L12-v2\"  # 384 dim\n",
    "    \n",
    "    print(f\"üìå Selected embedding model: {embedding_model}\\n\")\n",
    "    \n",
    "    # Initialize vector database with chosen model\n",
    "    vector_db = VectorDBManager(\n",
    "        collection_name=\"tech_docs_kb\",\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_model=embedding_model,\n",
    "        use_local_embeddings=True  # 100% local, no API calls\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ System components initialized!\")\n",
    "    print(\"üîí All processing runs locally - no data sent to external APIs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7471615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing Documents...\n",
      "\n",
      "  ‚úì RAG_Q&A.pdf: 17 chunks\n",
      "\n",
      "üì¶ Total chunks created: 17\n",
      "üíæ Average chunk size: 3581 characters\n"
     ]
    }
   ],
   "source": [
    "# Process documents and create chunks\n",
    "if not documents:\n",
    "    print(\"‚ö†Ô∏è  No documents to process. Please run Cell 7 first.\")\n",
    "else:\n",
    "    print(\"üìÑ Processing Documents...\\n\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = doc_processor.semantic_chunk(\n",
    "            doc['content'],\n",
    "            metadata={\n",
    "                'source': doc['source'],\n",
    "                'filename': doc['filename'],\n",
    "                'file_type': doc['file_type'],\n",
    "                'doc_type': 'technical_documentation'\n",
    "            }\n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"  ‚úì {doc['filename']}: {len(chunks)} chunks\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Total chunks created: {len(all_chunks)}\")\n",
    "    \n",
    "    if all_chunks:\n",
    "        avg_size = sum(c['metadata']['char_count'] for c in all_chunks) // len(all_chunks)\n",
    "        print(f\"üíæ Average chunk size: {avg_size} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8d75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Indexing documents in vector database...\n",
      "\n",
      "üîÑ Generating embeddings for 17 chunks...\n",
      "  ‚úì Processed batch 1/1\n",
      "‚úÖ Added 17 chunks to vector database\n",
      "\n",
      "‚úÖ All documents indexed successfully!\n",
      "üìä Total chunks in database: 17\n"
     ]
    }
   ],
   "source": [
    "# Add chunks to vector database\n",
    "if not all_chunks:\n",
    "    print(\"‚ö†Ô∏è  No chunks to index. Please run Cell 9 first.\")\n",
    "else:\n",
    "    print(\"üîç Indexing documents in vector database...\\n\")\n",
    "    \n",
    "    vector_db.add_documents(all_chunks)\n",
    "    \n",
    "    print(\"\\n‚úÖ All documents indexed successfully!\")\n",
    "    print(f\"üìä Total chunks in database: {vector_db.collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c973287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing RAG Agent...\n",
      "\n",
      "‚úÖ RAG Agent initialized\n",
      "\n",
      "‚úÖ RAG Agent ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize RAG Agent\n",
    "print(\"ü§ñ Initializing RAG Agent...\\n\")\n",
    "\n",
    "rag_agent = RAGAgent(vector_db, model=\"gpt-4o-mini\")\n",
    "rag_agent.initialize_agent()\n",
    "\n",
    "print(\"\\n‚úÖ RAG Agent ready to answer questions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7abdf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: How does chunk size impact retrieval accuracy?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "## Impact of Chunk Size on Retrieval Accuracy\n",
      "\n",
      "### Key Points on Chunk Size\n",
      "\n",
      "1. **Types of Chunking**:\n",
      "    - **Fixed-size chunking**: Prioritizes speed but may split sentences mid-thought, leading to loss of context which can negatively impact retrieval accuracy.\n",
      "    - **Semantic chunking**: Splits at topic boundaries, preserving meaning. Best for structured content where topics vary significantly.\n",
      "    - **Recursive chunking**: Initially attempts larger chunks and splits if necessary, maintaining natural boundaries such as paragraphs or sentences.\n",
      "    - **Hierarchical chunking**: Creates parent-child relationships for summarization, useful for long documents where multiple levels of detail are required.\n",
      "\n",
      "2. **Overlap Considerations**:\n",
      "    - **Overlapping chunks**: Useful when critical information might be split across boundaries. A recommended overlap (50-100 tokens) can help ensure that concepts are not lost.\n",
      "    - **Non-overlapping chunks**: Suitable for distinct sections to avoid redundancy and reduce storage requirements.\n",
      "\n",
      "### Retrieving Meaningful Information\n",
      "\n",
      "- The effectiveness of embeddings is directly affected by chunk boundaries. For instance, when a sentence is split improperly, the embed loses contextual meaning, thus degrading retrieval accuracy.\n",
      "- By including overlaps, complete thoughts can be preserved in multiple chunks, enhancing the accuracy of retrieval. For example, in a case where the sentence ‚ÄúAttention allows focusing on relevant parts‚Äù is split, without overlapping context, the system may retrieve incomplete meanings [{Source 1}].\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Chunk size and structure play a critical role in the retrieval accuracy of relevant information. Proper chunking strategies‚Äîwhether semantic, recursive, or hierarchical‚Äîensure that meaning is preserved and enhance the likelihood of accurate retrieval during searches.\n"
     ]
    }
   ],
   "source": [
    "# Test with a single query\n",
    "query = \"How does chunk size impact retrieval accuracy?\"  # Change this question\n",
    "\n",
    "print(f\"‚ùì Question: {query}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response = await rag_agent.query(query)\n",
    "print(f\"\\nü§ñ Answer:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9931d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question 1/3: What are the main topics covered in the documents?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "The main topics covered in the documents, particularly in the context of Retrieval-Augmented Generation (RAG), include:\n",
      "\n",
      "1. **Chunking Techniques**:\n",
      "   - **Fixed-size Chunking**: Simple and fast, but may break sentences mid-thought.\n",
      "   - **Semantic Chunking**: Identifies topic boundaries for coherent topic changes.\n",
      "   - **Recursive Chunking**: Attempts larger chunks first before splitting.\n",
      "   - **Hierarchical Chunking**: Structures wide documents into parent-child relationships.\n",
      "\n",
      "2. **Retrieval Techniques**:\n",
      "   - **Multi-hop and Chain-of-Thought Retrieval**: Breaks complex queries into reasoning steps, allowing for better structured retrieval.\n",
      "   - **Chunk Routing**: Directs queries to specific document subsets based on query type or metadata to improve relevance and speed.\n",
      "\n",
      "3. **RAG Workflows**:\n",
      "   - **Use of Orchestrators**: Handles multi-step workflows, state management, and integrates various tools to enhance RAG system efficiency.\n",
      "   - **Evaluation of RAG Systems**: Importance of testing with real user queries and diverse difficulty levels to build reliable evaluation datasets.\n",
      "\n",
      "4. **Guardrails in RAG**: Ensures safe and correct system behavior, preventing errors like hallucinations, enforcing context-only answers, and maintaining citation accuracy.\n",
      "\n",
      "5. **Prompts and Instructions**:\n",
      "   - Designing effective prompts that require citations and manage retrieval-augmented system prompting.\n",
      "\n",
      "6. **Retrieval Metrics**: Metrics like Recall@K, MRR (Mean Reciprocal Rank), nDCG (Normalized Discounted Cumulative Gain), to measure retrieval performance.\n",
      "\n",
      "These topics encompass various aspects of effectively managing and leveraging document retrieval and generation systems to ensure accurate, relevant, and well-structured outputs. For example, in chunking, techniques are tailored to preserve meaning while processing, and in retrieval, structured inquiry helps in obtaining concise answers through logical reasoning (Sources 1, 2, 3).\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Question 2/3: Can you summarize the key points?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "Here are the key points summarized from the retrieved context regarding retrieval-augmented generation (RAG) systems:\n",
      "\n",
      "### 1. Prompt Design for Citations\n",
      "- Clearly instruct models to cite sources in a specific format.\n",
      "- Use examples to demonstrate desired citation styles.\n",
      "- Validate outputs to ensure citations are included; regenerate if missing.\n",
      "\n",
      "### 2. Retrieval-Augmented Prompting\n",
      "- This approach builds prompts dynamically using relevant instructions based on user queries.\n",
      "- Failures may occur due to irrelevant or contradictory instructions or ambiguities in the query.\n",
      "\n",
      "### 3. Building Evaluation Datasets\n",
      "- Collect real user queries across diverse topics and identify ground truth answers.\n",
      "- Include a range of difficulty levels and negative examples to comprehensively test the system.\n",
      "\n",
      "### 4. Metrics for Retrieval Evaluation\n",
      "- **Recall@K**: Measures the percentage of relevant chunks found in top K results; useful when missing information is critical.\n",
      "- **Precision@K**: Evaluates the percentage of retrieved chunks that are relevant; important when quality matters.\n",
      "- **Mean Reciprocal Rank (MRR)**: Useful when the rank of the first relevant result is essential, such as in FAQ systems.\n",
      "- **Normalized Discounted Cumulative Gain (nDCG)**: Considers the relevance and position of results, suitable for ranking.\n",
      "\n",
      "### 5. Debugging Hallucinations\n",
      "- Identify whether issues stem from retrieval failure (missing relevant chunks) or generation failure (incorrect output from available chunks).\n",
      "- Improve prompts or retrieval mechanisms based on the type of failure.\n",
      "\n",
      "### 6. Chunking Strategies\n",
      "- **Fixed-size chunking**: Simple and fast but may disrupt sentence structure.\n",
      "- **Semantic chunking**: Preserves meaning by splitting at topic boundaries.\n",
      "- **Recursive and Hierarchical chunking**: Aimed at maintaining natural boundaries while summarizing longer documents.\n",
      "\n",
      "### 7. Embedding Models Selection\n",
      "- Choose between various embedding models (e.g., OpenAI, Cohere, Voyage, Jina) based on retrieval accuracy, cost, and specific needs, such as multilingual support or flexibility for customization.\n",
      "\n",
      "These points provide a comprehensive overview of effective practices and considerations in designing and evaluating RAG systems, as well as strategies for troubleshooting and optimizing performance.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Question 3/3: What technical information is available?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "### Available Technical Information\n",
      "\n",
      "1. **Retrieval Metrics**:\n",
      "   - **Recall@K**: Measures the percentage of relevant chunks found in the top K results. Use it when not missing relevant information is crucial.\n",
      "   - **Precision@K**: Measures what percentage of retrieved chunks are relevant. Important when the quality of retrieved items is prioritized over quantity.\n",
      "   - **Mean Reciprocal Rank (MRR)**: Indicates how high the first relevant result appears, suitable for scenarios where the top result‚Äôs accuracy is critical (e.g., FAQs).\n",
      "   - **Normalized Discounted Cumulative Gain (nDCG)**: Accounts for both position and relevance; useful in ranking scenarios where both matter [Source 1].\n",
      "\n",
      "2. **Identifying Failures**:\n",
      "   - **Retrieval Failure**: Occurs when relevant chunks are missing. Debug by inspecting embedding quality, query phrasing, and index parameters.\n",
      "   - **Generation Failure**: Happens when relevant chunks are present, but the output is incorrect. Can be improved by enforcing grounding and post-generation validation [Source 1].\n",
      "\n",
      "3. **Chunk-Routing**:\n",
      "   - Directs queries to specific document subsets based on type, category, or metadata. It enhances retrieval efficiency by classifying queries and routing to appropriate indexes [Source 2].\n",
      "\n",
      "4. **Orchestrators in RAG**:\n",
      "   - Tools like LangGraph and LangChain manage complex workflows, allowing for multi-step processing, conditional logic, and state management in RAG systems. They enable improved maintainability and error handling by abstracting boilerplate code [Source 2].\n",
      "\n",
      "5. **Guardrails in RAG**:\n",
      "   - These are rules to ensure systems behave safely and correctly, preventing hallucinations and enforcing context-based responses. They enhance reliability by checking outputs against retrieved context [Source 2].\n",
      "\n",
      "6. **Prompt Design**:\n",
      "   - Explicitly structure prompts to mandate citations, improving the model's reference to sources. Use examples and validate generated content to ensure adherence to expected format [Source 3].\n",
      "\n",
      "7. **Evaluation Dataset for RAG**:\n",
      "   - Build a dataset using diverse user queries with defined ground truths and associated document chunks, including negative examples. Labeling and creating diverse difficulty levels are essential for objective performance measurement [Source 3].\n",
      "\n",
      "### Example Code Snippet for Chunk-Routing\n",
      "```python\n",
      "def route_query_to_index(query):\n",
      "    query_type = classify_query(query)\n",
      "    if query_type == 'HR':\n",
      "        return search_in_index('HR_policy_index')\n",
      "    elif query_type == 'technical':\n",
      "        return search_in_index('technical_docs_index')\n",
      "    else:\n",
      "        return search_in_index('general_index')\n",
      "```\n",
      "\n",
      "This structured overview encapsulates key areas of technical information available regarding RAG (Retrieval-Augmented Generation) systems and methodologies. If further details are required on specific topics, please let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple queries\n",
    "# Customize these questions based on your documents\n",
    "test_queries = [\n",
    "    \"What are the main topics covered in the documents?\",\n",
    "    \"Can you summarize the key points?\",\n",
    "    \"What technical information is available?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question {i}/{len(test_queries)}: {query}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    response = await rag_agent.query(query)\n",
    "    print(f\"\\nü§ñ Answer:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4eb5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Interactive RAG System\n",
      "Type 'exit' or 'quit' to stop\n",
      "\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Interactive query loop\n",
    "print(\"üí¨ Interactive RAG System\")\n",
    "print(\"Type 'exit' or 'quit' to stop\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"‚ùì Your question: \")\n",
    "    \n",
    "    if user_query.lower() in ['exit', 'quit', '']:\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    print(\"\\nü§ñ Answer:\")\n",
    "    response = await rag_agent.query(user_query)\n",
    "    print(f\"{response}\\n\")\n",
    "    print(\"-\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f928e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Vector Database Statistics\n",
      "\n",
      "======================================================================\n",
      "Collection Name: tech_docs_kb\n",
      "Total Chunks: 17\n",
      "Embedding Model: all-MiniLM-L6-v2 (Open-Source)\n",
      "Distance Metric: Cosine Similarity\n",
      "Persist Directory: ./chroma_db\n",
      "üîí Running 100% locally (no external API calls)\n",
      "\n",
      "üìê Embedding Details:\n",
      "   Dimension: 384\n",
      "   Max Sequence Length: 256\n",
      "\n",
      "üìÑ Sample Chunk Preview:\n",
      "Content: R A G\n",
      "I n t e r v i e w\n",
      "Q u e s t i o n s\n",
      "N a r e s h  E d a g o t t i\n",
      "F o l l o w  F o r  M o r e...\n",
      "\n",
      "Metadata: {'filename': 'RAG_Q&A.pdf', 'chunk_id': 0, 'doc_type': 'technical_documentation', 'char_count': 98, 'file_type': 'pdf', 'source': 'docs\\\\RAG_Q&A.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Check vector database statistics\n",
    "print(\"üìä Vector Database Statistics\\n\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Collection Name: {vector_db.collection.name}\")\n",
    "print(f\"Total Chunks: {vector_db.collection.count()}\")\n",
    "print(f\"Embedding Model: {vector_db.embedding_model_name} (Open-Source)\")\n",
    "print(f\"Distance Metric: Cosine Similarity\")\n",
    "print(f\"Persist Directory: ./chroma_db\")\n",
    "print(f\"üîí Running 100% locally (no external API calls)\")\n",
    "\n",
    "# Model information\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_info = SentenceTransformer(vector_db.embedding_model_name)\n",
    "print(f\"\\nüìê Embedding Details:\")\n",
    "print(f\"   Dimension: {model_info.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Max Sequence Length: {model_info.max_seq_length}\")\n",
    "\n",
    "# Show sample chunk\n",
    "sample = vector_db.collection.peek(limit=1)\n",
    "if sample['documents']:\n",
    "    print(f\"\\nüìÑ Sample Chunk Preview:\")\n",
    "    print(f\"Content: {sample['documents'][0][:300]}...\")\n",
    "    print(f\"\\nMetadata: {sample['metadatas'][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "927af030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loaded Documents Statistics\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìÑ RAG_Q&A.pdf\n",
      "   Type: PDF\n",
      "   Size: 57,877 characters\n",
      "   Words: ~6,894\n",
      "   Lines: 7,663\n",
      "   Preview: R A G I n t e r v i e w Q u e s t i o n s N a r e s h  E d a g o t t i F o l l o w  F o r  M o r e    1.What  issues  occur  when  parsing  PDFs  with  multiple  layouts,  and  how  do  you  handle  t...\n",
      "\n",
      "======================================================================\n",
      "üìä Total Statistics:\n",
      "   Documents: 1\n",
      "   Total Characters: 57,877\n",
      "   Total Words: ~6,894\n",
      "   Chunks Created: 17\n"
     ]
    }
   ],
   "source": [
    "# View loaded documents statistics\n",
    "if documents:\n",
    "    print(\"üìö Loaded Documents Statistics\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        chars = len(doc['content'])\n",
    "        words = len(doc['content'].split())\n",
    "        lines = doc['content'].count('\\n') + 1\n",
    "        \n",
    "        total_chars += chars\n",
    "        total_words += words\n",
    "        \n",
    "        print(f\"\\nüìÑ {doc['filename']}\")\n",
    "        print(f\"   Type: {doc['file_type'].upper()}\")\n",
    "        print(f\"   Size: {chars:,} characters\")\n",
    "        print(f\"   Words: ~{words:,}\")\n",
    "        print(f\"   Lines: {lines:,}\")\n",
    "        \n",
    "        # Preview first 200 characters\n",
    "        preview = doc['content'][:200].replace('\\n', ' ')\n",
    "        print(f\"   Preview: {preview}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä Total Statistics:\")\n",
    "    print(f\"   Documents: {len(documents)}\")\n",
    "    print(f\"   Total Characters: {total_chars:,}\")\n",
    "    print(f\"   Total Words: ~{total_words:,}\")\n",
    "    print(f\"   Chunks Created: {len(all_chunks)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No documents loaded yet. Please run Cell 7 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4e8f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Direct Vector Search Test\n",
      "Query: your search term here\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìÑ Result 1:\n",
      "File: RAG_Q&A.pdf\n",
      "Chunk ID: 10\n",
      "Relevance Score: 0.1589\n",
      "\n",
      "Content:\n",
      "system\n",
      " \n",
      "retrieves\n",
      " \n",
      "initial\n",
      " \n",
      "chunks,\n",
      " \n",
      "identifies\n",
      " \n",
      "what\n",
      " \n",
      "additional\n",
      " \n",
      "information\n",
      " \n",
      "is\n",
      " \n",
      "needed,\n",
      " \n",
      "and\n",
      " \n",
      "retrieves\n",
      " \n",
      "again.\n",
      " \n",
      "This\n",
      " \n",
      "continues\n",
      " \n",
      "until\n",
      " \n",
      "sufficient\n",
      " \n",
      "information\n",
      " \n",
      "is\n",
      " \n",
      "gathered.\n",
      "\n",
      "Retrieval  chain-of-thought  breaks  complex  queries  into  reasoning  steps,  retrieving  informat...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 2:\n",
      "File: RAG_Q&A.pdf\n",
      "Chunk ID: 5\n",
      "Relevance Score: 0.1503\n",
      "\n",
      "Content:\n",
      "er:  Flat  index  (brute  force)  compares  query  against  every  vector,  giving  100%  recall  but  slow  for  large  \n",
      "datasets.\n",
      " \n",
      "Use\n",
      " \n",
      "only\n",
      " \n",
      "for\n",
      " \n",
      "small\n",
      " \n",
      "collections\n",
      " \n",
      "under\n",
      " \n",
      "10,000\n",
      " \n",
      "vectors.\n",
      "\n",
      "HNSW  builds  a  graph  structure  for  fast  approximate  search.  It  offers  great  speed  and ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 3:\n",
      "File: RAG_Q&A.pdf\n",
      "Chunk ID: 13\n",
      "Relevance Score: 0.1293\n",
      "\n",
      "Content:\n",
      "FAQ\n",
      " \n",
      "queries\n",
      " \n",
      "with\n",
      " \n",
      "24-hour\n",
      " \n",
      "expiration.\n",
      " \n",
      "Do\n",
      " \n",
      "not\n",
      " \n",
      "cache\n",
      " \n",
      "for\n",
      " \n",
      "user-specific\n",
      " \n",
      "queries\n",
      " \n",
      "like\n",
      " \n",
      "\"show\n",
      " \n",
      "my\n",
      " \n",
      "recent\n",
      " \n",
      "orders\"\n",
      " \n",
      "as\n",
      " \n",
      "results\n",
      " \n",
      "are\n",
      " \n",
      "personalized\n",
      " \n",
      "and\n",
      " \n",
      "change\n",
      " \n",
      "frequently.\n",
      "\n",
      "40.How  do  you  parallelize  retrieval  and  generation  to  minimize  latency?  \n",
      "Answer:  Run  re...\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test vector search directly (without agent)\n",
    "test_query = \"your search term here\"  # Change this\n",
    "\n",
    "print(f\"üîç Direct Vector Search Test\")\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = vector_db.search(test_query, n_results=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nüìÑ Result {i}:\")\n",
    "    print(f\"File: {result['metadata']['filename']}\")\n",
    "    print(f\"Chunk ID: {result['metadata']['chunk_id']}\")\n",
    "    print(f\"Relevance Score: {1 - result['distance']:.4f}\")\n",
    "    print(f\"\\nContent:\\n{result['content'][:300]}...\")\n",
    "    print(\"-\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72896fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  This cell is for clearing the database.\n",
      "Uncomment the code to actually clear it.\n"
     ]
    }
   ],
   "source": [
    "# Clear the database - USE WITH CAUTION!\n",
    "# Uncomment the lines below to clear all documents\n",
    "\n",
    "# vector_db.clear_collection()\n",
    "# print(\"üóëÔ∏è  Database cleared!\")\n",
    "# print(\"‚ö†Ô∏è  Run Cells 8-11 to reinitialize the system with documents.\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  This cell is for clearing the database.\")\n",
    "print(\"Uncomment the code to actually clear it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deb5524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Comparing Open-Source Embedding Models\n",
      "\n",
      "======================================================================\n",
      "Test query: 'What are the best practices for Python development?'\n",
      "\n",
      "üì¶ Testing: all-MiniLM-L6-v2\n",
      "   ‚úì Load time: 4.65s\n",
      "   ‚úì Encode time: 0.021s\n",
      "   ‚úì Dimension: 384\n",
      "   ‚úì Model size: ~1.5 KB per embedding\n",
      "\n",
      "üì¶ Testing: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Divakar Kumar\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[2025-12-10 23:42:56 - e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1729 - WARNING] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Load time: 63.88s\n",
      "   ‚úì Encode time: 0.121s\n",
      "   ‚úì Dimension: 768\n",
      "   ‚úì Model size: ~3.0 KB per embedding\n",
      "\n",
      "üì¶ Testing: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Divakar Kumar\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[2025-12-10 23:43:59 - e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1729 - WARNING] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Load time: 23.61s\n",
      "   ‚úì Encode time: 0.033s\n",
      "   ‚úì Dimension: 384\n",
      "   ‚úì Model size: ~1.5 KB per embedding\n",
      "\n",
      "üì¶ Testing: intfloat/e5-small-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Divakar Kumar\\.cache\\huggingface\\hub\\models--intfloat--e5-small-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[2025-12-10 23:44:22 - e:\\dev25\\Building-Agentic-AI\\MS_Agent_Framework\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1729 - WARNING] Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Load time: 22.93s\n",
      "   ‚úì Encode time: 0.035s\n",
      "   ‚úì Dimension: 384\n",
      "   ‚úì Model size: ~1.5 KB per embedding\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üí° Recommendation:\n",
      "   - Fast & Good: all-MiniLM-L6-v2\n",
      "   - Best Accuracy: BAAI/bge-base-en-v1.5\n",
      "   - Multilingual: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "# Compare different embedding models\n",
    "# This helps you choose the best model for your use case\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "print(\"üî¨ Comparing Open-Source Embedding Models\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_to_test = [\n",
    "    \"all-MiniLM-L6-v2\",           # Fast, general purpose\n",
    "    \"all-mpnet-base-v2\",          # Better accuracy\n",
    "    \"BAAI/bge-small-en-v1.5\",     # State-of-the-art, small\n",
    "    \"intfloat/e5-small-v2\",       # Efficient\n",
    "]\n",
    "\n",
    "test_text = \"What are the best practices for Python development?\"\n",
    "\n",
    "print(f\"Test query: '{test_text}'\\n\")\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    try:\n",
    "        print(f\"üì¶ Testing: {model_name}\")\n",
    "        \n",
    "        # Load model\n",
    "        start = time.time()\n",
    "        model = SentenceTransformer(model_name)\n",
    "        load_time = time.time() - start\n",
    "        \n",
    "        # Generate embedding\n",
    "        start = time.time()\n",
    "        embedding = model.encode(test_text)\n",
    "        encode_time = time.time() - start\n",
    "        \n",
    "        print(f\"   ‚úì Load time: {load_time:.2f}s\")\n",
    "        print(f\"   ‚úì Encode time: {encode_time:.3f}s\")\n",
    "        print(f\"   ‚úì Dimension: {len(embedding)}\")\n",
    "        print(f\"   ‚úì Model size: ~{model.get_sentence_embedding_dimension() * 4 / 1024:.1f} KB per embedding\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "print(\"   - Fast & Good: all-MiniLM-L6-v2\")\n",
    "print(\"   - Best Accuracy: BAAI/bge-base-en-v1.5\")\n",
    "print(\"   - Multilingual: paraphrase-multilingual-MiniLM-L12-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f009cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
